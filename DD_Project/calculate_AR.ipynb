{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for analyzing Insider tradings and the effects on stock prices\n",
    "Written by Thomas Niedermayer and Gunnar Sjúrðarson Knudsen, as a conjoined effort for an interdiscplinary project in Data Science.\n",
    "\n",
    "Supervisor: Wolfgang Aussenegg\n",
    "\n",
    "Co-Supervisor: Sascha Hunold\n",
    "\n",
    "Purpose of this notebook is to calculate the Abnormal returns, which will be analzsed in the notebook Statistics.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypotheses\n",
    "\n",
    "#### Gunnar\n",
    "\n",
    "1. Hypothesis 1: Insiders are able to earn significant abnormal returns in the first\n",
    "weeks after disclosure.\n",
    "2. Hypothesis 2: Trades of type “Purchase” are most explaining of abnormal return.\n",
    "“Sale” less so, and “Sale + Option” does not have an effect.\n",
    "3. Hypothesis 3: Directors have changed behaviour during the times of Covid.\n",
    "\n",
    "#### Tom\n",
    "\n",
    "1. Hypothesis 1: Insiders are able to earn significant abnormal returns in\n",
    "the first weeks after disclosure of relevant information.\n",
    "2. Hypothesis 2: Insiders are significantly good at avoiding risk indicated\n",
    "by market downturns after insiders selling shares.\n",
    "3. Hypothesis 3: Directors have changed behaviour during the times of\n",
    "covid: Hypotheses 1 and 2 can be answered with significantly different\n",
    "confidence before and during the pandemic.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define which analysis is run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-22T14:53:23.975437Z",
     "iopub.status.busy": "2022-11-22T14:53:23.975437Z",
     "iopub.status.idle": "2022-11-22T14:53:24.444893Z",
     "shell.execute_reply": "2022-11-22T14:53:24.444120Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tools import load_settings\n",
    "settings = load_settings()\n",
    "NAME = settings[\"NAME\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-22T14:53:24.448775Z",
     "iopub.status.busy": "2022-11-22T14:53:24.447725Z",
     "iopub.status.idle": "2022-11-22T14:53:29.113587Z",
     "shell.execute_reply": "2022-11-22T14:53:29.112614Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from tools import display_table\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "# custom functions\n",
    "import source.analyse_single_company as UASC\n",
    "from source import data_checks, determine_T0_T1_T2, cut_timeseries, calculate_coefficients\n",
    "import logging\n",
    "\n",
    "logging.getLogger().setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-22T14:53:29.116239Z",
     "iopub.status.busy": "2022-11-22T14:53:29.116239Z",
     "iopub.status.idle": "2022-11-22T14:53:29.771336Z",
     "shell.execute_reply": "2022-11-22T14:53:29.770143Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data locations\n",
    "DATA_LOCATION = f'data/{NAME}/'\n",
    "DATA_LOCATION_INSIDER_PROCESSED = DATA_LOCATION + 'processed/insider/'\n",
    "DATA_LOCATION_RI = DATA_LOCATION + 'processed/RI_discard/'\n",
    "\n",
    "# set plotting sizes\n",
    "tick_size = 15\n",
    "label_size = 20\n",
    "title_size = 30\n",
    "fig_height = 20\n",
    "\n",
    "investigation_periods = settings[\"investigation_periods\"]\n",
    "keys = list(investigation_periods.keys())\n",
    "interval_borders = [investigation_periods[keys[0]][0], investigation_periods[keys[1]][1], investigation_periods[keys[0]][1]]\n",
    "# Read in the summary data from \"CompaniesToExclude\" notebook\n",
    "summary_data = pd.read_csv(DATA_LOCATION + '/scraping_summary.csv', index_col=0)\n",
    "# Generate list of which companies to analyse\n",
    "isins_to_use = summary_data[summary_data['reason_to_exclude'] == 'None']['ISIN CODE'].to_list()\n",
    "display(summary_data)\n",
    "print(f'We want to reduce to {len(isins_to_use)} isins')\n",
    "\n",
    "_ri_location = DATA_LOCATION_RI\n",
    "_insider_location = DATA_LOCATION_INSIDER_PROCESSED\n",
    "\n",
    "# Get locations to read in\n",
    "file_locs_ = os.listdir(_ri_location)\n",
    "print(f'Found {len(file_locs_)} possible files to analyze')\n",
    "# Filter files for analysis, and append path:\n",
    "file_locs = [_ri_location + f for f in file_locs_ if f[:-7] in isins_to_use]\n",
    "print(f'We are left with {len(file_locs)} to analyze')\n",
    "\n",
    "# Actually read in the company information\n",
    "companies = []\n",
    "print(\"loading return series...\")\n",
    "for file_loc in tqdm(file_locs):\n",
    "    with open(file_loc, \"rb\") as f:\n",
    "        company = pickle.load(f)\n",
    "    companies.append(company)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Returns, Analyse Companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-22T14:53:29.778427Z",
     "iopub.status.busy": "2022-11-22T14:53:29.776997Z",
     "iopub.status.idle": "2022-11-22T14:53:31.029161Z",
     "shell.execute_reply": "2022-11-22T14:53:31.028168Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Returns\")\n",
    "returns_df = [c.return_index_df.company_return for c in companies]\n",
    "\n",
    "print(\"concatenate\")\n",
    "df_returns = pd.concat(returns_df, axis=1)\n",
    "df_return_index = pd.concat([c.return_index_df for c in companies], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise Mean Daily Returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-22T14:53:31.038396Z",
     "iopub.status.busy": "2022-11-22T14:53:31.035520Z",
     "iopub.status.idle": "2022-11-22T14:53:32.142961Z",
     "shell.execute_reply": "2022-11-22T14:53:32.141963Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plt.rc('font', family='serif')\n",
    "plt.rc('xtick')\n",
    "plt.rc('ytick')\n",
    "\n",
    "fig = plt.figure(figsize=(fig_height, 10))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "returns_companies = df_returns.mean(axis=1)\n",
    "returns_companies.plot(color=\"k\", linewidth=0.7)\n",
    "\n",
    "ax.set_xlabel('Time (Years)', fontsize=label_size)\n",
    "ax.set_ylabel('Mean Daily Return', fontsize=label_size)\n",
    "ax.set_title('Mean Daily Returns Over Time', fontsize=title_size)\n",
    "plt.xticks(fontsize=tick_size)\n",
    "plt.yticks(fontsize=tick_size)\n",
    "\n",
    "\n",
    "for int_ in interval_borders:\n",
    "    plt.axvline(x = int_, color = 'red', label = 'DD Event time', linewidth = 1)\n",
    "\n",
    "plt.savefig(DATA_LOCATION +\"visualisations/NYSE_daily_returns.svg\", dpi=600, bbox_inches='tight')\n",
    "plt.show()\n",
    "# get var for the intervals\n",
    "print(\"variance of the intervals\")\n",
    "print(f'{round(returns_companies.loc[interval_borders[0]:interval_borders[1]].var(),10):.20f}')\n",
    "print(f'{round(returns_companies.loc[interval_borders[1]:interval_borders[2]].var(), 10):.20f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove companies outside of the timeframe of interest\n",
    "Is now fixed upstream, but being kept here for reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-22T14:53:32.151465Z",
     "iopub.status.busy": "2022-11-22T14:53:32.151465Z",
     "iopub.status.idle": "2022-11-22T14:53:38.070761Z",
     "shell.execute_reply": "2022-11-22T14:53:38.069721Z"
    }
   },
   "outputs": [],
   "source": [
    "earliest_timestamp = list(investigation_periods.values())[0][0]\n",
    "latest_timestamp = list(investigation_periods.values())[-1][1]\n",
    "filings_removed = 0\n",
    "filings_total = 0\n",
    "\n",
    "for company in tqdm(companies):\n",
    "    insider_data_df = company.insider_data_df\n",
    "    filing_dates = insider_data_df.FilingDate.apply(lambda x: x.floor(\"d\"))\n",
    "    mask = (filing_dates >= earliest_timestamp) & (filing_dates <= latest_timestamp)\n",
    "    company.insider_data_df = company.insider_data_df[mask]\n",
    "    filings_removed += (~mask).sum()\n",
    "    filings_total += mask.shape[0]\n",
    "\n",
    "filings_remaining = filings_total - filings_removed\n",
    "print(\"Total filings: {}\".format(filings_total))\n",
    "print(\"Removed {} filings\".format(filings_removed))\n",
    "print(\"Remaining filings: {}\".format(filings_remaining))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate Filing Trade Lag Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-22T14:53:38.079390Z",
     "iopub.status.busy": "2022-11-22T14:53:38.079390Z",
     "iopub.status.idle": "2022-11-22T14:53:57.046368Z",
     "shell.execute_reply": "2022-11-22T14:53:57.045228Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(\"get filing lags ...\")\n",
    "\n",
    "lags = []\n",
    "for company in tqdm(companies):\n",
    "    lag = UASC.analyse_single_company(company)\n",
    "    lags.append(lag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-22T14:53:57.055542Z",
     "iopub.status.busy": "2022-11-22T14:53:57.054538Z",
     "iopub.status.idle": "2022-11-22T14:54:00.871908Z",
     "shell.execute_reply": "2022-11-22T14:54:00.871908Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "filing_trade_lags = sum(lags, [])\n",
    "\n",
    "lag_in_hours = np.asarray(filing_trade_lags)\n",
    "negative_lag_mask = lag_in_hours < 0\n",
    "positive_lag = lag_in_hours[~negative_lag_mask]\n",
    "in_21_days = positive_lag < 21*24\n",
    "relevant_lag = positive_lag[in_21_days]\n",
    "print(f\"Negative lag for {negative_lag_mask.sum()} out of {len(negative_lag_mask)} trades.\")\n",
    "print(f\"Lag longer than 21 days for {len(positive_lag) - len(relevant_lag)} out of {len(negative_lag_mask)} trades.\")\n",
    "print(f\"Eligible trades: {len(relevant_lag)} out of {len(negative_lag_mask)} trades.\")\n",
    "\n",
    "fig = plt.figure(figsize=(fig_height, 7))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "plt.hist(np.log10(positive_lag), bins=\"auto\")\n",
    "ax.set_xlabel('Time (Log(Hours))', fontsize=label_size)\n",
    "ax.set_ylabel('Trades', fontsize=label_size)\n",
    "ax.set_title('Distribution of Log Lag Times between Filing and Trade', fontsize=title_size)\n",
    "\n",
    "plt.xticks(fontsize=tick_size)\n",
    "plt.yticks(fontsize=tick_size)\n",
    "\n",
    "plt.axvline(x = 0, color = 'red', label = 'Zero', linewidth = 1)\n",
    "plt.axvline(x = np.log10(21*24), color = 'red', label = 'Threshold', linewidth = 1)\n",
    "plt.savefig(DATA_LOCATION +\"visualisations/log_transformed_lags.svg\", dpi=600, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "without_outliers = positive_lag[positive_lag < 24*21]\n",
    "\n",
    "fig = plt.figure(figsize=(fig_height, 5))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "plt.hist(without_outliers, bins=\"auto\")\n",
    "plt.xticks(np.arange(0, max(without_outliers) + 1, 24))\n",
    "ax.set_xlabel('Time (Hours)', fontsize=label_size)\n",
    "ax.set_ylabel('Trades', fontsize=label_size)\n",
    "ax.set_title('Distribution of Lag Times between Filing and Trade', fontsize=title_size)\n",
    "\n",
    "plt.xticks(fontsize=tick_size)\n",
    "plt.yticks(fontsize=tick_size)\n",
    "\n",
    "plt.savefig(DATA_LOCATION +\"visualisations/lags_without_outliers.svg\", dpi=600, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop trades with lags longer than 21 days or negative lags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-22T14:54:00.876503Z",
     "iopub.status.busy": "2022-11-22T14:54:00.874737Z",
     "iopub.status.idle": "2022-11-22T14:54:02.322853Z",
     "shell.execute_reply": "2022-11-22T14:54:02.322853Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "removed_lags = 0\n",
    "remaining_lags = 0\n",
    "\n",
    "for lags_c, company in tqdm(zip(lags, companies)):\n",
    "    lags_c = np.asarray(lags_c)\n",
    "    mask_eligible = (lags_c >= 0) & (lags_c <= 21*24)\n",
    "    company.insider_data_df = company.insider_data_df[mask_eligible]\n",
    "    removed_lags += (~mask_eligible).sum()\n",
    "    remaining_lags += mask_eligible.sum()\n",
    "\n",
    "print(f\"Total trades: {removed_lags + remaining_lags}\")\n",
    "print(f\"Removed {removed_lags} trades.\")\n",
    "print(f\"Remaining {remaining_lags} trades.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstrate process for a single event"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define windows\n",
    "\n",
    "#### Our data contains multiple companies. A single company contains multiple filings and each filing is an event\n",
    "\n",
    "![alt text](assets/images/windows.png \"Title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-22T14:54:02.328014Z",
     "iopub.status.busy": "2022-11-22T14:54:02.327016Z",
     "iopub.status.idle": "2022-11-22T14:54:02.338953Z",
     "shell.execute_reply": "2022-11-22T14:54:02.338953Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "## Constants defining how long both Estimation Window and Event Window are\n",
    "### Probably also input parameters to a function call, as we need loops later...\n",
    "L1_length = 100\n",
    "L2_length = 41 # TODO +-20 days = 40 days, right?\n",
    "L2_length = L2_length -1 # this is so our calculation works as before, could be done cleaner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate ARs\n",
    "\n",
    "### Now that we have seen the process for one single filing, let us do the same for all filings in all companies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-22T14:54:02.344937Z",
     "iopub.status.busy": "2022-11-22T14:54:02.344937Z",
     "iopub.status.idle": "2022-11-22T14:56:12.956724Z",
     "shell.execute_reply": "2022-11-22T14:56:12.953922Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize testing\n",
    "logging.getLogger().setLevel(logging.ERROR)\n",
    "# Helpers\n",
    "multiind, data, data_errors = [], [], []\n",
    "estimation_window_market_return_list, event_window_market_return_list, eps_list = [], [], []\n",
    "n_companies = len(companies)\n",
    "\n",
    "\n",
    "#for j in tqdm(range(len(companies[:200]))):\n",
    "for j in tqdm(range(len(companies))):\n",
    "    # Get information from said company\n",
    "    company = companies[j]\n",
    "    company_return = company.return_index_df\n",
    "    \n",
    "    n_filings = len(company.insider_data_df)\n",
    "    # Go through all filings\n",
    "    for i in company.insider_data_df.FilingDate.index:\n",
    "\n",
    "        # Find our event date from filing\n",
    "        filing_date = company.insider_data_df.FilingDate[i]\n",
    "        event_timestamp = filing_date.floor(\"d\")\n",
    "\n",
    "        checks = data_checks.run(L1_length, L2_length, event_timestamp, company_return)\n",
    "        if checks:\n",
    "            #print(checks[1])\n",
    "            data_errors.append(checks[0])\n",
    "            continue\n",
    "   \n",
    "        ## Proceed to find periods\n",
    "        T0_, T1_, T2_, T0, T1, T2, ERRORS, msg = determine_T0_T1_T2.run(L1_length, L2_length, event_timestamp, company_return)\n",
    "        if ERRORS:\n",
    "            #print(msg)\n",
    "            data_errors.append(ERRORS)\n",
    "            continue\n",
    "            \n",
    "        ## Cut timeseries to the relevant periods, and split them\n",
    "        windows = cut_timeseries.run(company_return, T0, T1, T2)\n",
    "        estimation_window_market_return, estimation_window_company_return,event_window_market_return, event_window_company_return = windows\n",
    "        alpha, beta, eps = calculate_coefficients.run(estimation_window_market_return, estimation_window_company_return) # TODO change back to calculate_coefficients.run or check validity\n",
    "        ## Calculate the abnormal returns\n",
    "        abnormal_return = event_window_company_return - alpha - beta*event_window_market_return\n",
    "        \n",
    "        ## Append to results\n",
    "        estimation_window_market_return_list.append(estimation_window_market_return)\n",
    "        event_window_market_return_list.append(event_window_market_return)\n",
    "        eps_list.append(eps)\n",
    "        multiind.append((company.ticker, i, company.insider_data_df.TradeType[i], event_timestamp))\n",
    "        data.append(abnormal_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-22T14:56:12.969064Z",
     "iopub.status.busy": "2022-11-22T14:56:12.968575Z",
     "iopub.status.idle": "2022-11-22T14:56:18.862097Z",
     "shell.execute_reply": "2022-11-22T14:56:18.860872Z"
    }
   },
   "outputs": [],
   "source": [
    "# process and save abnormal returns\n",
    "df_abnormal_returns = pd.DataFrame.from_records([d.reset_index(drop=True) for d in data])\n",
    "df_abnormal_returns.index = pd.MultiIndex.from_tuples(multiind, names=[\"Company\", \"i\", \"TradeType\", \"event_timestamp\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-22T14:56:18.869875Z",
     "iopub.status.busy": "2022-11-22T14:56:18.868401Z",
     "iopub.status.idle": "2022-11-22T14:56:18.970695Z",
     "shell.execute_reply": "2022-11-22T14:56:18.970486Z"
    }
   },
   "outputs": [],
   "source": [
    "df_abnormal_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-22T14:56:18.975199Z",
     "iopub.status.busy": "2022-11-22T14:56:18.974170Z",
     "iopub.status.idle": "2022-11-22T14:56:24.161745Z",
     "shell.execute_reply": "2022-11-22T14:56:24.160377Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df_estimation_window_market_return = pd.DataFrame.from_records([d.reset_index(drop=True) for d in estimation_window_market_return_list])\n",
    "df_estimation_window_market_return.index = pd.MultiIndex.from_tuples(multiind, names=[\"Company\", \"i\", \"TradeType\", \"event_timestamp\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-22T14:56:24.169149Z",
     "iopub.status.busy": "2022-11-22T14:56:24.168334Z",
     "iopub.status.idle": "2022-11-22T14:56:30.034200Z",
     "shell.execute_reply": "2022-11-22T14:56:30.032864Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df_event_window_market_return = pd.DataFrame.from_records([d.reset_index(drop=True) for d in event_window_market_return_list])\n",
    "df_event_window_market_return.index = pd.MultiIndex.from_tuples(multiind, names=[\"Company\", \"i\", \"TradeType\", \"event_timestamp\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-22T14:56:30.042028Z",
     "iopub.status.busy": "2022-11-22T14:56:30.041576Z",
     "iopub.status.idle": "2022-11-22T14:56:35.984057Z",
     "shell.execute_reply": "2022-11-22T14:56:35.983049Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df_eps = pd.DataFrame.from_records([d.reset_index(drop=True) for d in eps_list])\n",
    "df_eps.index = pd.MultiIndex.from_tuples(multiind, names=[\"Company\", \"i\", \"TradeType\", \"event_timestamp\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show the reasons filings were dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-22T14:56:35.988129Z",
     "iopub.status.busy": "2022-11-22T14:56:35.987038Z",
     "iopub.status.idle": "2022-11-22T14:56:36.004646Z",
     "shell.execute_reply": "2022-11-22T14:56:36.003287Z"
    }
   },
   "outputs": [],
   "source": [
    "errors_df = pd.DataFrame.from_records(data_errors)\n",
    "errors_agg = errors_df.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-22T14:56:36.012705Z",
     "iopub.status.busy": "2022-11-22T14:56:36.011707Z",
     "iopub.status.idle": "2022-11-22T14:56:36.038614Z",
     "shell.execute_reply": "2022-11-22T14:56:36.036345Z"
    }
   },
   "outputs": [],
   "source": [
    "# add earlier filtering\n",
    "errors_agg[\"event not in overall time frame\"] = filings_removed\n",
    "errors_agg[\"negative lags\"] = negative_lag_mask.sum()\n",
    "errors_agg[\"more than 21 days lag\"] = len(positive_lag) - len(relevant_lag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-22T14:56:36.051187Z",
     "iopub.status.busy": "2022-11-22T14:56:36.049780Z",
     "iopub.status.idle": "2022-11-22T14:56:36.070394Z",
     "shell.execute_reply": "2022-11-22T14:56:36.067471Z"
    }
   },
   "outputs": [],
   "source": [
    "errors_agg = pd.DataFrame(errors_agg)\n",
    "errors_agg.index.name = \"Reason\"\n",
    "errors_agg.columns = [\"N Filings\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-22T14:56:36.084575Z",
     "iopub.status.busy": "2022-11-22T14:56:36.083260Z",
     "iopub.status.idle": "2022-11-22T14:56:36.175223Z",
     "shell.execute_reply": "2022-11-22T14:56:36.175223Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "errors_agg_print = errors_agg.astype(int).reset_index().copy()\n",
    "errors_agg_print.insert(0, \" \", len(errors_agg_print)*[\" \"])\n",
    "print(display_table(errors_agg_print))\n",
    "\n",
    "errors_agg_print_b = errors_agg_print.iloc[:,1:].sum(axis=0).values\n",
    "errors_agg_print_b = pd.DataFrame([\"Before Filtering\", \" \"] + list(errors_agg_print_b)).transpose()\n",
    "errors_agg_print_b.index = [0]\n",
    "errors_agg_print_b.columns = [\"        \"] + list(errors_agg_print.columns)\n",
    "print(display_table(errors_agg_print_b))\n",
    "\n",
    "print(errors_agg.sum())\n",
    "print(filings_remaining-errors_agg.sum())\n",
    "print(filings_remaining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-22T14:56:36.178391Z",
     "iopub.status.busy": "2022-11-22T14:56:36.178391Z",
     "iopub.status.idle": "2022-11-22T14:56:36.194081Z",
     "shell.execute_reply": "2022-11-22T14:56:36.191832Z"
    }
   },
   "outputs": [],
   "source": [
    "print(errors_agg.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-22T14:56:36.201082Z",
     "iopub.status.busy": "2022-11-22T14:56:36.200083Z",
     "iopub.status.idle": "2022-11-22T14:56:36.690206Z",
     "shell.execute_reply": "2022-11-22T14:56:36.688847Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# if dir does not exist create it\n",
    "if not os.path.exists(f\"data/{NAME}/calculate_AR_results/\"):\n",
    "    os.mkdir(f\"data/{NAME}/calculate_AR_results/\")\n",
    "\n",
    "\n",
    "df_abnormal_returns.to_pickle(f\"data/{NAME}/calculate_AR_results/df_abnormal_returns.pkl\")\n",
    "df_estimation_window_market_return.to_pickle(f\"data/{NAME}/calculate_AR_results/df_estimation_window_market_return.pkl\")\n",
    "df_event_window_market_return.to_pickle(f\"data/{NAME}/calculate_AR_results/df_event_window_market_return.pkl\")\n",
    "df_eps.to_pickle(f\"data/{NAME}/calculate_AR_results/df_eps.pkl\")\n",
    "\n",
    "with open(f\"data/{NAME}/calculate_AR_results/companies.pkl\", \"wb\") as f:\n",
    "    pickle.dump(companies, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}