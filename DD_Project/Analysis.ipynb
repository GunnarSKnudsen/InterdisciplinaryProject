{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "source": [
    "# Notebook for analyzing Insider tradings and the effects on stock prices\n",
    "Written by Thomas Niedermayer and Gunnar Sjúrðarson Knudsen, as a conjoined effort for an interdiscplinary project in Data Science.\n",
    "\n",
    "Supervisor: Wolfgang Aussenegg\n",
    "\n",
    "Co-Supervisor: Sascha Hunold\n",
    "\n",
    "Purpose of this notebook is XXX\n",
    "\n",
    "## Remaining todos:\n",
    "\n",
    "* T1_ vs T1!!!\n",
    "* A lot!\n",
    "* Clean up library loading - Pycharm it!!\n",
    "* Figure out which custom functions we are still using\n",
    "  - * How much would Thomas hate me, if I moved \"Main\" to a notebook as well?\n",
    "* Figure out if different hypotheses should be tested based on \"NAME\" - or do both do all analysis?\n",
    "* Refactor - we have data locations in two different varialbes (CAPS and preceeding underscore)\n",
    "* I MIGHT have deleted too much from data_checks.run\n",
    "* main.py: check which imports are used\n",
    "* Currently have two different datasets for the ReturnIndex data - with (linear) interpolation, as well as skipping rows that don't exist in market and company.\n",
    "* Figure out what to do, when event date not in the dataset. Could still \"just\" do closest possible, provided that trades occur around the date.\n",
    "* Rerun main.py for Niedermayer - needs the re-processed dataset with market information!\n",
    "* What do we do when tickers are non-unique!? I think this is a nasty that breaks more than we know\n",
    "* document get_all_directors_dealings_async\n",
    "* document analyse_single_company\n",
    "* Are outliers \"Significant\"? Wilcoxon compared to t-test\n",
    "\n",
    "#### Ask Prof. Aussenegg\n",
    "* How to compare before and after pandemic? subsample to get the same sample size and 2 sample ttest?\n",
    "* Is it fair to compare like before pandemic with during pandemic? when there is an estimation window in the pre pandemic time and the event window is in the pandemic time\n",
    "* : T1 19 or 20 BEFORE event? (e.g. 40 or 41 day window)?\n",
    "* Are we allowed to persist the preprocessed data for this study?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypotheses\n",
    "\n",
    "#### Gunnar\n",
    "\n",
    "1. Hypothesis 1: Insiders are able to earn significant abnormal returns in the first\n",
    "weeks after disclosure.\n",
    "2. Hypothesis 2: Trades of type “Purchase” are most explaining of abnormal return.\n",
    "“Sale” less so, and “Sale + Option” does not have an effect.\n",
    "3. Hypothesis 3: Directors have changed behaviour during the times of Covid.\n",
    "\n",
    "#### Tom\n",
    "\n",
    "1. Hypothesis 1: Insiders are able to earn significant abnormal returns in\n",
    "the first weeks after disclosure of relevant information.\n",
    "2. Hypothesis 2: Insiders are significantly good at avoiding risk indicated\n",
    "by market downturns after insiders selling shares.\n",
    "3. Hypothesis 3: Directors have changed behaviour during the times of\n",
    "covid: Hypotheses 1 and 2 can be answered with significantly different\n",
    "confidence before and during the pandemic.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define which analysis is run\n",
    "Add a name here. This affects which data is read in, as well as which analysis are done?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "NAME = \"Knudsen\" # \"Niedermayer\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import requests\n",
    "import locale\n",
    "from pandas.io.json import json_normalize\n",
    "import os\n",
    "from os.path import exists\n",
    "import sys\n",
    "import io\n",
    "from tqdm import tqdm\n",
    "\n",
    "# custom functions\n",
    "import source.read_tickers_and_isins as URTI\n",
    "import source.get_directors_dealings as UGDD\n",
    "import source.get_timeseries as UGT\n",
    "import source.analyze_get_summary_of_data as AGSOD\n",
    "import source.preprocess_directors_dealings as UPDD\n",
    "import source.preprocess_timeseries as UPTS\n",
    "import source.preprocess_timeseries_from_excel as UPTFE\n",
    "import source.analyse_single_company as UASC\n",
    "import source.calculate_daily_returns_for_period as CDRFP\n",
    "from source import data_checks, determine_T0_T1_T2, cut_timeseries, calculate_coefficients\n",
    "import logging\n",
    "\n",
    "# Custom - stolen from the macro analysis\n",
    "from source.data_checks import DataSizeException\n",
    "from source.determine_T0_T1_T2 import TimeSeriesMismatchException\n",
    "\n",
    "logging.getLogger().setLevel(logging.WARNING)\n",
    "#plt.rcParams.update({\n",
    "#    'text.usetex': True,\n",
    "#    'font.family': 'serif',\n",
    "#    'axes.labelsize': 10,\n",
    "#    'font.size': 11,\n",
    "#    'legend.fontsize': 10,\n",
    "#    'xtick.labelsize': 10,\n",
    "#    'ytick.labelsize': 10,\n",
    "#    'figure.dpi': 96\n",
    "#})\n",
    "\n",
    "\n",
    "#plt.style.use(\"seaborn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Read in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Data locations\n",
    "DATA_LOCATION = f'data/{NAME}/'\n",
    "DATA_LOCATION_INSIDER_PROCESSED = DATA_LOCATION + 'processed/insider/'\n",
    "DATA_LOCATION_RI = DATA_LOCATION + 'processed/RI_discard/'\n",
    "\n",
    "## Fairly certain that these are no longer used\n",
    "#DATA_LOCATION_INSIDER_RAW = DATA_LOCATION + 'raw/insider/'\n",
    "#DATA_LOCATION_TIME_SERIES_RAW = DATA_LOCATION + 'raw/timeseries/'\n",
    "#DATA_LOCATION_TIME_SERIES_PROCESSED = DATA_LOCATION + 'processed/timeseries/'\n",
    "\n",
    "# Read in the summary data from \"CompaniesToExclude\" notebook\n",
    "summary_data = pd.read_csv(DATA_LOCATION + '/scraping_summary.csv', index_col=0)\n",
    "# Generate list of which companies to analyse\n",
    "isins_to_use = summary_data[summary_data['reason_to_exclude'] == 'None']['ISIN CODE'].to_list()\n",
    "display(summary_data)\n",
    "print(f'We want to reduce to {len(isins_to_use)} isins')\n",
    "\n",
    "\n",
    "## Not sure why we do this - maybe refactor\n",
    "_ri_location = DATA_LOCATION_RI\n",
    "_insider_location = DATA_LOCATION_INSIDER_PROCESSED\n",
    "\n",
    "# Get locations to read in\n",
    "file_locs_ = os.listdir(_ri_location)\n",
    "print(f'Found {len(file_locs_)} possible files to analyze')\n",
    "# Filter files for analysis, and append path:\n",
    "file_locs = [_ri_location + f for f in file_locs_ if f[:-7] in isins_to_use]\n",
    "print(f'We are left with {len(file_locs)} to analyze')\n",
    "\n",
    "# Actually read in the company information\n",
    "companies = []\n",
    "print(\"loading return series...\")\n",
    "for file_loc in tqdm(file_locs):\n",
    "    with open(file_loc, \"rb\") as f:\n",
    "        company = pickle.load(f)\n",
    "    companies.append(company)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Calculate Returns, Analyse Companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Returns\")\n",
    "returns_df = [c.return_index_df.company_return for c in companies]\n",
    "\n",
    "print(\"concatenate\")\n",
    "df_returns = pd.concat(returns_df, axis=1)\n",
    "df_return_index = pd.concat([c.return_index_df for c in companies], axis=1)\n",
    "\n",
    "print(\"analyse ...\")\n",
    "pickles = os.listdir(DATA_LOCATION_RI)\n",
    "\n",
    "#WTF?! Why does this break??\n",
    "#ISINs = [rick[:-7] for rick in pickles][:100] # TODO remove slice\n",
    "#ISINs = [rick for rick in pickles]\n",
    "\n",
    "ISINs =  [f[:-7] for f in file_locs_ if f[:-7] in isins_to_use]\n",
    "print(len(ISINs))\n",
    "\n",
    "outputs = []\n",
    "for isin in tqdm(ISINs):\n",
    "    outputs.append(UASC.analyse_single_company(isin, DATA_LOCATION_RI, DATA_LOCATION_INSIDER_PROCESSED))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Visualise Mean Daily Returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.rc('font', family='serif')\n",
    "plt.rc('xtick')\n",
    "plt.rc('ytick')\n",
    "\n",
    "fig = plt.figure(figsize=(7, 5))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "returns_companies = df_returns.mean(axis=1)\n",
    "returns_companies.plot(color=\"k\", linewidth=0.7)\n",
    "\n",
    "ax.set_xlabel('Time (Years)')\n",
    "ax.set_ylabel('Mean Daily Return')\n",
    "ax.set_title('Mean Daily Returns Over Time')\n",
    "\n",
    "interval_borders = [\"2020-03-01\"] # TODO see if it makes sense to actually take first of Feb\n",
    "\n",
    "for int_ in interval_borders:\n",
    "    plt.axvline(x = int_, color = 'red', label = 'DD Event time', linewidth = 1)\n",
    "\n",
    "plt.savefig(DATA_LOCATION +\"visualisations/NYSE_daily_returns.jpg\", dpi=600)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Investigate Filing Trade Lag Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sum_returns = sum([x[0] for x in outputs], [])\n",
    "filing_trade_lags = sum([x[1] for x in outputs], [])\n",
    "\n",
    "lag_in_hours = np.asarray(filing_trade_lags)\n",
    "negative_lag_mask = lag_in_hours < 0\n",
    "positive_lag = lag_in_hours[~negative_lag_mask]\n",
    "in_21_days = positive_lag < 21*24\n",
    "relevant_lag = positive_lag[in_21_days]\n",
    "print(f\"Negative lag for {negative_lag_mask.sum()} out of {len(negative_lag_mask)} trades.\")\n",
    "print(f\"Lag longer than 21 days for {len(positive_lag) - len(relevant_lag)} out of {len(negative_lag_mask)} trades.\")\n",
    "print(f\"Eligible trades: {len(relevant_lag)} out of {len(negative_lag_mask)} trades.\")\n",
    "\n",
    "fig = plt.figure(figsize=(7, 5))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "plt.hist(np.log(positive_lag), bins=\"auto\")\n",
    "ax.set_xlabel('Time (Log(Hours))')\n",
    "ax.set_ylabel('Trades')\n",
    "ax.set_title('Distribution of Log Lag Times between Filing and Trade')\n",
    "\n",
    "plt.savefig(DATA_LOCATION +\"visualisations/log_transformed_lags.jpg\", dpi=600)\n",
    "plt.show()\n",
    "\n",
    "without_outliers = positive_lag[positive_lag < 24*21]\n",
    "\n",
    "fig = plt.figure(figsize=(7, 5))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "plt.hist(without_outliers, bins=\"auto\")\n",
    "plt.xticks(np.arange(0, max(without_outliers) + 1, 24))\n",
    "ax.set_xlabel('Time (Hours)')\n",
    "ax.set_ylabel('Trades')\n",
    "ax.set_title('Distribution of Lag Times between Filing and Trade')\n",
    "\n",
    "plt.savefig(DATA_LOCATION +\"visualisations/lags_without_outliers.jpg\", dpi=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Demonstrate process for a single event"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define windows\n",
    "\n",
    "#### Our data contains multiple companies. A single company contains multiple filings and each filing is an event\n",
    "\n",
    "![alt text](assets/images/windows.png \"Title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "## Constants defining how long both Estimation Window and Event Window are\n",
    "### Probably also input parameters to a function call, as we need loops later...\n",
    "L1_length = 100\n",
    "L2_length = 40 # TODO +-20 days = 40 days, right?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix a company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "logging.getLogger().setLevel(logging.DEBUG)\n",
    "\n",
    "if NAME == \"Knudsen\":\n",
    "    company_index = -87\n",
    "elif NAME == \"Niedermayer\":\n",
    "    company_index = -11\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "company = companies[company_index]\n",
    "print(company)\n",
    "company_return = company.return_index_df\n",
    "insider_data_df = pd.read_csv(_insider_location + company.ticker + '.csv', index_col=0, parse_dates=['FilingDate', 'TradeDate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "insider_data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Fix an event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This date will be moved to a loop\n",
    "## Define which periods we are looking at.\n",
    "\n",
    "if NAME == \"Knudsen\":\n",
    "    event_index = 60\n",
    "elif NAME == \"Niedermayer\":\n",
    "    event_index = -200\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "    \n",
    "event_timestamp = insider_data_df.FilingDate.iloc[event_index].floor(\"d\") \n",
    "print(\"event timestamp: \", event_timestamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Technical Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data_checks.run(L1_length, L2_length, event_timestamp, company_return)#, market_timeseries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#company_return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Determine T0, T1 and T2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "T0_, T1_, T2_, T0, T1, T2, ERROR, msg = determine_T0_T1_T2.run(L1_length, L2_length, event_timestamp, company_return)#, market_timeseries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "print(event_timestamp)\n",
    "company_return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Abnormal and Normal Returns\n",
    "\n",
    "![alt text](assets/images/return_estimation.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Cut return timeseries into correct periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "windows = cut_timeseries.run(company_return, T0, T1, T2)\n",
    "estimation_window_market_return, estimation_window_company_return, event_window_market_return, event_window_company_return = windows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Calculate coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "alpha, beta = calculate_coefficients.run(estimation_window_market_return, estimation_window_company_return)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### The Abnormal Return\n",
    "This is the last step of the whole process for one event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "company_return = event_window_company_return\n",
    "market_return = event_window_market_return\n",
    "estimated_return = alpha + beta*market_return\n",
    "abnormal_return = company_return - estimated_return\n",
    "print(abnormal_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_and_estimated = pd.DataFrame({\"company\":event_window_company_return, \"market\":estimated_return})\n",
    "company_and_market = pd.DataFrame({\"company\": estimation_window_company_return, \"market\":estimation_window_market_return})\n",
    "df_to_plot = pd.concat([company_and_market, company_and_estimated])\n",
    "df_to_plot.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "Company_name = company.name\n",
    "\n",
    "# Estimations\n",
    "est_estimation = estimation_window_market_return * beta + alpha\n",
    "est_event = event_window_market_return * beta + alpha\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "estimation_window_market_return.plot(color = 'black', alpha = 0.6, linewidth=4, label = 'Market Return (Estimation Window)')\n",
    "event_window_market_return.plot(color = 'black', alpha = 0.9, linewidth=4, label = 'Market Return (Event Window)')\n",
    "\n",
    "estimation_window_company_return.plot(color = 'blue', alpha = 0.6, linewidth = 4, label = f'{Company_name} Return (Estimation Window)')\n",
    "event_window_company_return.plot(color = 'blue', alpha = 0.9, linewidth = 4, label = f'{Company_name} Return (Event Window)')\n",
    "\n",
    "plt.axvline(x = event_timestamp, color = 'red', label = 'DD Event time', linewidth = 5)\n",
    "plt.ylabel(f'Daily Returns of Company {Company_name} and Market')\n",
    "est_estimation.plot(color = 'green', label = f'Regression Estimate for {Company_name}', alpha = 0.8)\n",
    "est_event.plot(color = 'green', label = f'Regression Estimate for {Company_name}', alpha = 1)\n",
    "\n",
    "plt.axvspan(T0, T1, ymin = 0.05, ymax = 0.95, facecolor='black', alpha=0.1, label = 'Estimation Window', edgecolor='g', linewidth=5)\n",
    "plt.axvspan(T1, T2, ymin = 0.05, ymax = 0.95, facecolor='black', alpha=0.2, label = 'Event Window', edgecolor='r', linewidth=5)\n",
    "plt.legend(bbox_to_anchor = (1.0, 1), loc = 'upper left')\n",
    "\n",
    "plt.title(f'Show how company \"{Company_name}\" moves around the event, compared to the market')\n",
    "plt.show()\n",
    "print(\"I'm impressed! It looks like a five-year-old drew this plot in paint\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVENT_INDEX = 20 # because right now 20 [0,..19] are before the event"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Macro Analysis\n",
    "\n",
    "### Now that we have seen the process for one single filing, let us do the same for all filings in all companies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize testing\n",
    "logging.getLogger().setLevel(logging.ERROR)\n",
    "\n",
    "# Helpers\n",
    "multiind, data, data_errors = [], [], []\n",
    "n_companies = len(companies)\n",
    "\n",
    "# this is the interval where filings are interesting to us\n",
    "earliest_timestamp = pd.Timestamp(\"2018-01-01\")\n",
    "latest_timestamp = pd.Timestamp(\"2021-12-31\")\n",
    "\n",
    "#for j in tqdm(range(len(companies[:200]))):\n",
    "for j in tqdm(range(len(companies))):\n",
    "    # Get information from said company\n",
    "    company = companies[j]\n",
    "    company_return = company.return_index_df\n",
    "    \n",
    "    # Read in insider trading informations:\n",
    "    filename = _insider_location + company.ticker + '.csv'\n",
    "    filename = filename.replace(\" \", \"+\") # HBB+WI in files and HBB WI as Ticker in base Excel, TODO should be fixed in base data\n",
    "    insider_data_df = pd.read_csv(filename, index_col=0, parse_dates=['FilingDate', 'TradeDate'])\n",
    "    \n",
    "    \n",
    "    n_filings = len(insider_data_df)\n",
    "    # Go through all filings\n",
    "    for i in insider_data_df.FilingDate.index:\n",
    "        \n",
    "        # Find our event date from filing\n",
    "        filing_date = insider_data_df.FilingDate[i]\n",
    "        event_timestamp = filing_date.floor(\"d\")\n",
    "\n",
    "        # if the timestamp is too early or too late we skip\n",
    "        if event_timestamp < earliest_timestamp or event_timestamp > latest_timestamp:\n",
    "            #print(\"skipping, filing is too early or too late\")\n",
    "            continue\n",
    "            \n",
    "        #print(f\"working on company {j}/{n_companies} named {company.name}, filing {i}/{n_filings}\")\n",
    "\n",
    "        # do the process for one filing\n",
    "        ## See if it's possible\n",
    "        checks = data_checks.run(L1_length, L2_length, event_timestamp, company_return)\n",
    "        if checks:\n",
    "            #print(checks[1])\n",
    "            data_errors.append(checks[0])\n",
    "            continue\n",
    "   \n",
    "        ## Proceed to find periods\n",
    "        T0_, T1_, T2_, T0, T1, T2, ERRORS, msg = determine_T0_T1_T2.run(L1_length, L2_length, event_timestamp, company_return)#, market_timeseries)\n",
    "        if ERRORS:\n",
    "            #print(msg)\n",
    "            data_errors.append(ERRORS)\n",
    "            continue\n",
    "            \n",
    "        ## Cut timeseries to the relevant periods, and split them\n",
    "        windows = cut_timeseries.run(company_return, T0, T1, T2)\n",
    "        estimation_window_market_return, estimation_window_company_return, event_window_market_return, event_window_company_return = windows\n",
    "        alpha, beta = calculate_coefficients.run(estimation_window_market_return, estimation_window_company_return)\n",
    "\n",
    "        ## Calculate the abnormal returns\n",
    "        abnormal_return = event_window_company_return - alpha - beta*event_window_market_return\n",
    "        \n",
    "        ## Append to results set\n",
    "        multiind.append((company.ticker, i, insider_data_df.TradeType[i], event_timestamp))\n",
    "        data.append(abnormal_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# process abnormal returns\n",
    "df_abnormal_returns = pd.DataFrame.from_records([d.reset_index(drop=True) for d in data])\n",
    "df_abnormal_returns.index = pd.MultiIndex.from_tuples(multiind, names=[\"Company\", \"i\", \"TradeType\", \"event_timestamp\"])\n",
    "df_abnormal_returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show the reasons filings were dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "errors_df = pd.DataFrame.from_records(data_errors)\n",
    "errors_df.sum(axis=0).plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Having a look at all trade types together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_abnormal_returns.loc[:,:,[\"P - Purchase\", \"S - Sale\", \"S - Sale+OE\"],:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "a = df_abnormal_returns.loc[:,:,[\"P - Purchase\", \"S - Sale\", \"S - Sale+OE\"],:].groupby(level=[2]).sum().transpose().plot(figsize=(20, 10), fontsize=15)\n",
    "plt.plot(np.zeros(len(df_abnormal_returns.columns)), color=\"black\", linewidth=0.5)\n",
    "a.set_title(\"Abnormal Returns of all Trade Types\",fontsize=30)\n",
    "a.set_xlabel(\"Days\", fontsize=20)\n",
    "a.set_ylabel(\"Abnormal Return\", fontsize=20)\n",
    "plt.axvline(x = EVENT_INDEX, color = 'red', label = 'DD Event time', linewidth = 1.5)\n",
    "plt.savefig(f\"data/{NAME}/visualisations/AR_all_tradetypes.png\", dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_abnormal_returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think OE and OptEx means option exercise. \"to exercise\" means to put into effect the right to buy or sell the underlying security that is specified in the options contract.\" Can we be sure that the action does not shift the market, and the swing in return is really due to new information? Are these trades public, so maybe they are used as a signal for traders?\n",
    "\n",
    "### Boxplot of the sum over all companies's AR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "df_abnormal_returns.groupby(level=[2]).sum().transpose().plot.box(rot=90, figsize=(20, 10), fontsize=15)\n",
    "ax.set_title(\"Boxplots of the Abnormal Returns for each Trade Type\",fontsize=30)\n",
    "ax.set_xlabel(\"Trade Type\", fontsize=20)\n",
    "ax.set_ylabel(\"Abnormal Return\", fontsize=20)\n",
    "\n",
    "plt.savefig(f\"data/{NAME}/visualisations/AR_all_tradetypes_boxplot.png\", dpi=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "![alt text](assets/images/time_agg.png)\n",
    "\n",
    "In our case it is not company i, but filing i\n",
    "\n",
    "Types of trade to pick from:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "types = list(set([x[2] for x in df_abnormal_returns.index]))\n",
    "types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the type of the trades to investigate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "type_ = \"P - Purchase\" # \"S - Sale\"\n",
    "df_abnormal_returns_type = df_abnormal_returns.loc[:,:,type_]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The index describes the company and the index of the filing in the compnay, the columns represent the days in the event window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_abnormal_returns_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# TODO Infinity Values idk why, should check out why they are there upstream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "mask = (-df_abnormal_returns_type == np.Inf) |(df_abnormal_returns_type == np.Inf)\n",
    "df_abnormal_returns_type[mask] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "mask.sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "CAR = df_abnormal_returns_type.sum(axis=1)\n",
    "CAR.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](assets/images/cross_sectional_agg.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "AR_bar = df_abnormal_returns_type.mean(axis=0) \n",
    "AR_bar.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "CAR_bar = AR_bar.sum()\n",
    "CAR_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# TODO var_CAR_bar = Does the definition make sense? It seems like we take the var of a scalar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](assets/images/cross_sectional_agg2.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "CAR_bar_2 = CAR.mean()\n",
    "CAR_bar_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# var car TODO not sure what just sigma means."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "source": [
    "## Statistics\n",
    "\n",
    "Check if CAR mean = 0 (t-test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "tt = stats.ttest_1samp(CAR, popmean=0)\n",
    "tt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if CAR median = 0 (wilcoxon signed rank test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "stats.wilcoxon(CAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "types = types\n",
    "investigation_periods = {\n",
    "    \"overall\": (pd.Timestamp(\"2018-01-01\"), pd.Timestamp(\"2021-12-31\")),\n",
    "    \"pre-pandemic\": (pd.Timestamp(\"2018-01-01\"), pd.Timestamp(\"2020-02-29\")),\n",
    "    \"pandemic\": (pd.Timestamp(\"2020-03-01\"), pd.Timestamp(\"2021-12-31\")),\n",
    "}\n",
    "\n",
    "multiind_p, data_p = [], []\n",
    "\n",
    "event_day_ranges = {\n",
    "    \"before\": (0,20), # TODO for now we drop the filing day itself\n",
    "    \"after\": (21,41)\n",
    "}\n",
    "\n",
    "\n",
    "for type_ in types:\n",
    "    for per in investigation_periods.keys():\n",
    "        for side in event_day_ranges.keys():\n",
    "                \n",
    "            event_day_range = event_day_ranges[side]\n",
    "            df_abnormal_returns_type = df_abnormal_returns.loc[:,:,type_]\n",
    "            per_left,per_right = investigation_periods[per]\n",
    "\n",
    "\n",
    "            timestamps = df_abnormal_returns_type.index.get_level_values(2)\n",
    "            mask = (timestamps >= per_left) & (timestamps <= per_right)\n",
    "            df_AR_type_per = df_abnormal_returns_type[mask]\n",
    "            if not len(df_AR_type_per):\n",
    "                print(f\"skipping iteration because of 0 datapoints {(type_, per, side)}\")\n",
    "                continue\n",
    "            df_AR_type_per_side = df_AR_type_per.iloc[:,event_day_range[0]:event_day_range[1]]\n",
    "            CAR = df_AR_type_per_side.sum(axis=1)\n",
    "\n",
    "            pvalue_ttest = round(stats.ttest_1samp(CAR, popmean=0).pvalue, 10)\n",
    "            pvalue_wilcoxon = round(stats.wilcoxon(CAR).pvalue, 10)\n",
    "            mean = CAR.mean()\n",
    "            median = CAR.median()\n",
    "            \n",
    "            multiind_p.append((type_, per, side))\n",
    "            data_p.append((mean, pvalue_ttest, median, pvalue_wilcoxon, len(df_AR_type_per)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process abnormal returns\n",
    "df_p = pd.DataFrame.from_records(data_p)\n",
    "df_p.index = pd.MultiIndex.from_tuples(multiind_p, names=[\"TradeType\", \"Period\", \"Side of the event\"])\n",
    "df_p.columns = [\"mean\", \"ttest pvalue\", \"median\", \"wilcoxon pvalue\", \"sample_size\"]\n",
    "df_p = df_p.sort_values(\"ttest pvalue\")\n",
    "df_p.to_csv(f\"data/{NAME}/tests_result.csv\")\n",
    "df_p"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
